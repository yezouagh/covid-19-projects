{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "covid-19-face-mask-detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rSMk9sahYKaj",
        "y3LpxIwLaMnL",
        "tjjKIhNSa7Z1",
        "mZ7euxZwhCHZ",
        "c2l8O6WPkewr",
        "L8Trflg8sIXu",
        "A1NcRNxtrFIo",
        "jTsVxo25tYAi",
        "1iaUcqMTtqgA",
        "t8-FPAS01NuD",
        "qIxd4xfcFexS"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPL2UKc27HdoEDzzD1M3J1H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yezouagh/covid-19-projects/blob/master/covid_19_face_mask_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSMk9sahYKaj",
        "colab_type": "text"
      },
      "source": [
        "# COVID-19 face mask detector training script with Keras and TensorFlow\n",
        "\n",
        "To accomplish this task, we’ll be fine-tuning the **MobileNet V2 architecture**, a highly efficient architecture that can be applied to embedded devices with limited computational capacity (ex., Raspberry Pi, Google Coral, NVIDIA Jetson Nano, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3LpxIwLaMnL",
        "colab_type": "text"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-IzXpEyaaba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjjKIhNSa7Z1",
        "colab_type": "text"
      },
      "source": [
        "## Download Face-Mask-Detector DataSet From Kaggle\n",
        "\n",
        "When you Create your API key in Kaggle it will be automatically downloaded then upload it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njeeb69Gch1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json  # set permission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3X8S9bubLc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets download -d andrewmvd/face-mask-detection -p /content/drive/My\\ Drive/Colab\\ Notebooks/face-masks-dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ7euxZwhCHZ",
        "colab_type": "text"
      },
      "source": [
        "### Unzip The DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZHYJmtOhBqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q /content/drive/My\\ Drive/Colab\\ Notebooks/face-masks-dataset/face-mask-detection.zip -d /content/drive/My\\ Drive/Colab\\ Notebooks/face-masks-dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2l8O6WPkewr",
        "colab_type": "text"
      },
      "source": [
        "### List All files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYM7XdWyZHBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing\n",
        "\n",
        "# running this will list all files under the input directory\n",
        "direc = '/content/drive/My Drive/Colab Notebooks/face-masks-dataset/'\n",
        "import os\n",
        "for dirname, _, filenames in os.walk(direc+'images'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM74nya3knLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lxml.etree as et\n",
        "\n",
        "for file in os.listdir(direc+\"annotations\"):\n",
        "    xml = et.parse(direc+\"annotations/\"+file)\n",
        "    pretty = et.tostring(xml, encoding=\"unicode\", pretty_print=True)\n",
        "    print('--------------------------------------------------------------------------------------------')\n",
        "    print(pretty)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8Trflg8sIXu",
        "colab_type": "text"
      },
      "source": [
        "## Loading and Preprocessing Trainning Data\n",
        "\n",
        "At this point, we’re ready to load and pre-process our training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l6K2wFfHH_v",
        "colab_type": "text"
      },
      "source": [
        "###Method to detect face #1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKas0HgqDOek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def detectFaceOpenCVDnn(net, frame):\n",
        "    conf_threshold = 0.8\n",
        "    found = False\n",
        "    frameOpencvDnn = frame.copy()\n",
        "    (frameHeight,frameWidth) = frameOpencvDnn.shape[:2]\n",
        "    # construct a blob from the image\n",
        "    # applying mean subtraction of values (104, 177, 123) for each blue, green and red channels correspondingly.\n",
        "    blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], False, False)\n",
        "    \n",
        "    # pass the blob through the network and obtain the face detections\n",
        "    #print(\"[INFO] computing face detections...\")\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "    for i in range(detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "        if confidence > conf_threshold and  not found:\n",
        "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
        "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
        "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
        "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
        "            #if (x2 - x1) > 40 :\n",
        "            #  found = True\n",
        "            frameOpencvDnn = frame[y1:y2, x1:x2]\n",
        "            \n",
        "            cv2_imshow(frameOpencvDnn)\n",
        "    \n",
        "    return frameOpencvDnn\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O_DqFEJG-i2",
        "colab_type": "text"
      },
      "source": [
        "###Method to detect face #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psdd-_rT5KmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dlib\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "def detectFace(frame):\n",
        "    frameGray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    found = False\n",
        "    #(frameHeight,frameWidth) = frameGray.shape[:2]\n",
        "    faces = detector(frameGray, 0)\n",
        "    if len(faces) > 0:\n",
        "        for face in faces:\n",
        "          x1 = face.left()\n",
        "          y1 = face.top()\n",
        "          x2 = face.right()\n",
        "          y2 = face.bottom()\n",
        "          if (x2 - x1) > 40 and not found:\n",
        "            found = True\n",
        "            frame = frame[y1:y2, x1:x2]\n",
        "          #display(x1,y1,x2,y2,frame2)\n",
        "        #cv2_imshow(frame2)\n",
        "    \n",
        "    return frame"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AZPw2OiHQ8S",
        "colab_type": "text"
      },
      "source": [
        "### Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fxIH4mf3Ti6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c885d499-56e1-4576-dd02-3d70fa2b9165"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer,LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as et\n",
        "import numpy as np\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "import random as rand\n",
        "from imutils import paths\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "direc = '/content/drive/My Drive/Colab Notebooks/face-masks-dataset/'\n",
        "face = direc+\"face_detector\"\n",
        "# load our serialized face detector model from disk\n",
        "print(\"[INFO] loading face detector model...\")\n",
        "modelFile = os.path.sep.join([face, \"res10_300x300_ssd_iter_140000_fp16.caffemodel\"])\n",
        "configFile = os.path.sep.join([face, \"deploy.prototxt\"])\n",
        "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
        "# extract a single face from a given photograph\n",
        "def extract_faces_labels(filename, required_size=(224, 224)):\n",
        "    labels = []\n",
        "    imgs = []\n",
        "    xml = et.parse(filename)\n",
        "    root = xml.getroot()\n",
        "    img = root[1].text\n",
        "    h, w = root[2][0].text, root[2][1].text\n",
        "    img = cv2.imread(direc+\"images/\"+img)\n",
        "    img = cv2.resize(img, (int(h),int(w)))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    for i in range(4, len(root)):\n",
        "        labels.append(root[i][0].text)\n",
        "        info = [int(each.text) for each in root[i][5]]\n",
        "        face = img[info[1]:info[3], info[0]:info[2]]\n",
        "        face = cv2.resize(face, required_size)\n",
        "        face = img_to_array(face)\n",
        "        face = preprocess_input(face)\n",
        "        #display('first',face)\n",
        "        imgs.append(face)\n",
        "    return imgs, labels\n",
        "\n",
        "def load_extra_data(directory):\n",
        "  print(\"[INFO] loading extra images...\")\n",
        "  imagePaths = list(paths.list_images(directory))\n",
        "  faces = []\n",
        "  labels = []\n",
        "\n",
        "  # loop over the image paths\n",
        "  for imagePath in imagePaths:\n",
        "    # extract the class label from the filename\n",
        "    label = imagePath.split(os.path.sep)[-2]\n",
        "    face = cv2.imread(imagePath)\n",
        "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    # load the input image (224x224) and preprocess it\n",
        "    face = detectFace(face)\n",
        "    #display(face.shape)\n",
        "    if face.shape[0] > 0 and face.shape[1] > 0 and face.shape[2] > 0:\n",
        "      face = cv2.resize(face, (224, 224))\n",
        "      #cv2_imshow(face)\n",
        "      face = img_to_array(face)\n",
        "      face = preprocess_input(face)\n",
        "      # update the data and labels lists, respectively\n",
        "      faces.append(face)\n",
        "     #display(label,imagePath)\n",
        "      labels.append(label)\n",
        "    #break\n",
        "  return faces, labels\n",
        "\n",
        "# load images and extract faces for all images in a directory\n",
        "def load_dataset(directory):\n",
        "    x = []\n",
        "    y = []\n",
        "    # enumerate files\n",
        "    for file in os.listdir(directory+\"annotations/\"):\n",
        "        # get face\n",
        "        faces, labels = extract_faces_labels(directory+\"annotations/\" + file)\n",
        "        # store\n",
        "        x.extend(faces)\n",
        "        y.extend(labels)\n",
        "        #break\n",
        "    extra_faces, extra_labels = load_extra_data(directory+\"extra/\")\n",
        "    x.extend(extra_faces)\n",
        "    y.extend(extra_labels)\n",
        "    return np.asarray(x), np.asarray(y)\n",
        "\n",
        "#extra_faces, extra_labels = load_extra_data(direc+\"extra/\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading face detector model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcv_Jwrcnsf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data, labels = load_dataset(direc)\n",
        "\n",
        "# convert the data and labels to NumPy arrays\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "labels = np.array(labels)\n",
        "# labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9GWqrkRRfAk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "45f675d4-402e-4e98-c75f-07f567c6748a"
      },
      "source": [
        "#len(labels)\n",
        "#extra_labels = np.array(extra_labels)\n",
        "#data.shape\n",
        "#data\n",
        "labels[2]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'with_mask'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOJE8SsCnu_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform LabelEncoder encoding on the labels\n",
        "lb = LabelEncoder()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW5ED1fonxJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We’ll be applying on-the-fly mutations to our images in an effort to improve generalization. \n",
        "# This is known as data augmentation\n",
        "\n",
        "aug = ImageDataGenerator(\n",
        "    zoom_range=0.05,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1NcRNxtrFIo",
        "colab_type": "text"
      },
      "source": [
        "## Prepare MobileNetV2 for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZkNk3ghn0aY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# construct the head of the model that will be placed on top of the\n",
        "# the base model\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(64, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.6)(headModel)\n",
        "headModel = Dense(3, activation=\"softmax\")(headModel)\n",
        "\n",
        "# place the head FC model on top of the base model (this will become\n",
        "# the actual model we will train)\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they will\n",
        "# *not* be updated during the first training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M--ar48aoH1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# partition the data into training and testing splits using 70% of\n",
        "# the data for training and the remaining 30% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "\ttest_size=0.3, stratify=labels, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTsVxo25tYAi",
        "colab_type": "text"
      },
      "source": [
        "## Compiling the model\n",
        "\n",
        "With our data prepared and model architecture in place for fine-tuning, we’re now ready to compile and train our face mask detector network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhMFGi9soDxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize the initial learning rate, number of epochs to train for,\n",
        "# and batch size\n",
        "INIT_LR = 1e-4\n",
        "EPOCHS = 50\n",
        "BS = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oowU9K3BoJ4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "# train the head of the network\n",
        "print(\"[INFO] training head...\")\n",
        "H = model.fit(\n",
        "\taug.flow(trainX, trainY, batch_size=BS),\n",
        "\tsteps_per_epoch=len(trainX) // BS,\n",
        "\tvalidation_data=(testX, testY),\n",
        "\tvalidation_steps=len(testX) // BS,\n",
        "\tepochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iaUcqMTtqgA",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating the model\n",
        "\n",
        "Once training is complete, we’ll evaluate the resulting model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5RDPedKoQEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make predictions on the testing set\n",
        "\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predIdxs = model.predict(testX, batch_size=32)\n",
        "\n",
        "# for each image in the testing set we need to find the index of the\n",
        "# label with corresponding largest predicted probability\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# show a nicely formatted classification report\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "\ttarget_names=lb.classes_))\n",
        "\n",
        "# serialize the model to disk\n",
        "print(\"[INFO] saving mask detector model...\")\n",
        "model.save(direc+'mask_detector3.model', save_format=\"h5\")\n",
        "#model_index = model_index + 1\n",
        "\n",
        "# plot the training loss and accuracy\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"center right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8-FPAS01NuD",
        "colab_type": "text"
      },
      "source": [
        "## USAGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtER2Vev6fGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "direc = '/content/drive/My Drive/Colab Notebooks/face-masks-dataset/'\n",
        "face = direc+\"face_detector\"\n",
        "modelname = direc+\"mask_detector3.model\"\n",
        "\n",
        "classes = [\"mask_weared_incorrect\",\"with_mask\", \"without_mask\"]\n",
        "Colors = [ (255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
        "\n",
        "# load our serialized face detector model from disk\n",
        "print(\"[INFO] loading face detector model...\")\n",
        "modelFile = os.path.sep.join([face, \"res10_300x300_ssd_iter_140000_fp16.caffemodel\"])\n",
        "configFile = os.path.sep.join([face, \"deploy.prototxt\"])\n",
        "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
        "\n",
        "# load the face mask detector model from disk\n",
        "print(\"[INFO] loading face mask detector model...\")\n",
        "model = load_model(modelname)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO2fxnosFzWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detectFaceOpenCVDnn(net, frame):\n",
        "    frameOpencvDnn = frame.copy()\n",
        "    (frameHeight,frameWidth) = frameOpencvDnn.shape[:2]\n",
        "    conf_threshold = 0.5\n",
        "    # construct a blob from the image\n",
        "    # applying mean subtraction of values (104, 177, 123) for each blue, green and red channels correspondingly.\n",
        "    blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], False, False)\n",
        "    \n",
        "    # pass the blob through the network and obtain the face detections\n",
        "    #print(\"[INFO] computing face detections...\")\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "    for i in range(detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "        if confidence > conf_threshold:\n",
        "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
        "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
        "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
        "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
        "\t\t        # display the label and bounding box rectangle on the output\n",
        "\t\t        # frame\n",
        "            # extract the face ROI, convert it from BGR to RGB channel\n",
        "\t\t        # ordering, resize it to 224x224, and preprocess it\n",
        "            face = frame[y1:y2, x1:x2]\n",
        "            face = cv2.resize(face, (224, 224))\n",
        "            face = img_to_array(face)\n",
        "            face = preprocess_input(face)\n",
        "            face = np.expand_dims(face, axis=0)\n",
        "\t\t        # pass the face through the model to determine if the face\n",
        "\t\t        # has a mask or not\n",
        "            pred = model.predict(face)\n",
        "\t\t        # determine the class label and color we'll use to draw\n",
        "\t\t        # the bounding box and text\n",
        "            predIdxs = np.argmax(pred, axis=1)\n",
        "            label = classes[predIdxs[0]]\n",
        "            color = Colors[predIdxs[0]]\n",
        "            #display(pred,predIdxs,predIdxs[0])\n",
        "\t\t        # include the probability in the label\n",
        "            percentage = \"{:.2f}%\".format(max(pred[0]) * 100)\n",
        "            cv2.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), color, int(round(frameHeight/150)), 8)\n",
        "            cv2.putText(frameOpencvDnn, label, (x1 - 8, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.40, color, 1, cv2.LINE_AA)\n",
        "            cv2.putText(frameOpencvDnn, percentage, (x1, y1 - 8), cv2.FONT_HERSHEY_SIMPLEX, 0.40, color, 1, cv2.LINE_AA)\n",
        "    \n",
        "    # show the output image\n",
        "    cv2_imshow(frameOpencvDnn)        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNTMtbnvierT",
        "colab_type": "text"
      },
      "source": [
        "###USAGE on Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz5RPuwS_5kS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the input image from disk, clone it, and grab the image spatial\n",
        "# dimensions\n",
        "#imagename = direc+\"Examples/example_01.png\"\n",
        "imagename = '/content/drive/My Drive/Colab Notebooks/face-masks-dataset/images/maksssksksss387.png'\n",
        "image = cv2.imread(imagename)\n",
        "detectFaceOpenCVDnn(net,image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZPUqhqpiKEA",
        "colab_type": "text"
      },
      "source": [
        "###Usage for Video Stream"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVUCEj4nTWTF",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=1.0):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      // show the video in the HTML element\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // prints the logs to cell\n",
        "      let jsLog = function(abc) {\n",
        "        document.querySelector(\"#output-area\").appendChild(document.createTextNode(`${abc}... `));\n",
        "      }\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      // await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      for (let i = 0; i < 5; i++) {\n",
        "        const canvas = document.createElement('canvas');\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        img = canvas.toDataURL('image/jpeg', quality);\n",
        "\n",
        "        jsLog(i + \"sending\")\n",
        "        // Call a python function and send this image\n",
        "        google.colab.kernel.invokeFunction('notebook.run_algo', [img], {});\n",
        "        jsLog(i + \"SENT\")\n",
        "\n",
        "        // wait for X miliseconds second, before next capture\n",
        "        await new Promise(resolve => setTimeout(resolve, 250));\n",
        "      }\n",
        "\n",
        "      stream.getVideoTracks()[0].stop(); // stop video stream\n",
        "    }\n",
        "    ''')\n",
        "  display(js) # make the provided HTML, part of the cell\n",
        "  data = eval_js('takePhoto({})'.format(quality)) # call the takePhoto() JavaScript function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku3eVT4ji1lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import dlib\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "direc = '/content/drive/My Drive/Colab Notebooks/face-masks-dataset/'\n",
        "face = direc+\"face_detector\"\n",
        "modelname = direc+\"mask_detector2.model\"\n",
        "conf_threshold = 0.5\n",
        "classes = [\"mask_weared_incorrect\",\"with_mask\", \"without_mask\"]\n",
        "Colors = [ (255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
        "\n",
        "# load our serialized face detector model from disk\n",
        "print(\"[INFO] loading face detector model...\")\n",
        "modelFile = os.path.sep.join([face, \"res10_300x300_ssd_iter_140000_fp16.caffemodel\"])\n",
        "configFile = os.path.sep.join([face, \"deploy.prototxt\"])\n",
        "net = cv2.dnn.readNetFromCaffe(configFile, modelFile)\n",
        "\n",
        "# load the face mask detector model from disk\n",
        "print(\"[INFO] loading face mask detector model...\")\n",
        "model = load_model(modelname)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YGqn0XgT3pK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import IPython\n",
        "from google.colab import output\n",
        "from google.colab.patches import cv2_imshow\n",
        "import time\n",
        "import sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import base64\n",
        "import logging\n",
        "\n",
        "def data_uri_to_img(uri):\n",
        "  \"\"\"convert base64image to numpy array\"\"\"\n",
        "  try:\n",
        "    image = base64.b64decode(uri.split(',')[1], validate=True)\n",
        "    # make the binary image, a PIL image\n",
        "    image = Image.open(BytesIO(image))\n",
        "    # convert to numpy array\n",
        "    image = np.array(image, dtype=np.uint8); \n",
        "    return image\n",
        "  except Exception as e:\n",
        "    logging.exception(e);print('\\n')\n",
        "    return None\n",
        "\n",
        "def run_algo(imgB64):\n",
        "  \"\"\"\n",
        "  in Colab, run_algo function gets invoked by the JavaScript, that sends N images every second\n",
        "\n",
        "  params:\n",
        "    image: image\n",
        "  \"\"\"\n",
        "  image = data_uri_to_img(imgB64)  \n",
        "  if image is None:\n",
        "    print(\"At run_algo(): image is None.\")\n",
        "    return\n",
        "\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  try:\n",
        "    # Run detection\n",
        "    detectFaceOpenCVDnn(net,image)\n",
        "  except Exception as e:\n",
        "    logging.exception(e)\n",
        "    print('\\n'+e)\n",
        "# register this function, so JS code could call this\n",
        "output.register_callback('notebook.run_algo', run_algo)\n",
        "\n",
        "# put the JS code in cell and run it\n",
        "take_photo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIxd4xfcFexS",
        "colab_type": "text"
      },
      "source": [
        "## MTCNN FACE DETECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXypcTlrCt67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mtcnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaWOaxMABUba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# face detection with mtcnn on a photograph\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.patches import Rectangle\n",
        "from matplotlib.patches import Circle\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "\n",
        "# draw an image with detected objects\n",
        "def draw_image_with_boxes(filename, result_list):\n",
        "\t# load the image\n",
        "\tdata = pyplot.imread(filename)\n",
        "\t# plot the image\n",
        "\tpyplot.imshow(data)\n",
        "\t# get the context for drawing boxes\n",
        "\tax = pyplot.gca()\n",
        "\t# plot each box\n",
        "\tfor result in result_list:\n",
        "\t\t# get coordinates\n",
        "\t\tx, y, width, height = result['box']\n",
        "\t\t# create the shape\n",
        "\t\trect = Rectangle((x, y), width, height, fill=False, color='red')\n",
        "\t\t# draw the box\n",
        "\t\tax.add_patch(rect)\n",
        "\t\t# draw the dots\n",
        "\t\tfor key, value in result['keypoints'].items():\n",
        "\t\t\t# create and draw dot\n",
        "\t\t\tdot = Circle(value, radius=2, color='red')\n",
        "\t\t\tax.add_patch(dot)\n",
        "\t# show the plot\n",
        "\tpyplot.show()\n",
        "\n",
        "filename = '/content/example_03.png'\n",
        "# load image from file\n",
        "pixels = cv2.imread(filename)\n",
        "# create the detector, using default weights\n",
        "detector = MTCNN()\n",
        "# detect faces in the image\n",
        "faces = detector.detect_faces(pixels)\n",
        "# display faces on the original image\n",
        "draw_image_with_boxes(filename, faces)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CuzMrKTgasb",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Download face detector model\n",
        "\n",
        "import requests\n",
        "CHUNK_SIZE = 32768\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "    session = requests.Session()\n",
        "    response = session.get(URL, params={'id': id}, stream=True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = {'id': id, 'confirm': token}\n",
        "        response = session.get(URL, params=params, stream=True)\n",
        "\n",
        "    save_response_content(response, destination)\n",
        "\n",
        "\n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "    return None\n",
        "\n",
        "    \n",
        "def save_response_content(response, destination):\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:  # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "\n",
        "download_file_from_google_drive('1is7Ldv9ASYNcrv2GyXS7EaV58UaqhuFQ', direc+'rfcn_resnet101_widerface_91674.tar.gz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XOryyDkmatM",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Unzip Model\n",
        " import tarfile\n",
        " def extract_tar_file(zip_file_name, destination):\n",
        "        zip_ref = tarfile.TarFile.open(zip_file_name, 'r')\n",
        "        zip_ref.extractall(destination)\n",
        "        zip_ref.close()\n",
        "\n",
        "extract_tar_file(direc+'rfcn_resnet101_widerface_91674.tar.gz',direc+'rfcn_resnet101_widerface_91674')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ails15sps0V",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Remove Folders or Files\n",
        "!rm -r '/content/drive/My Drive/Colab Notebooks/face-masks-dataset/rfcn_resnet101_widerface_91674/'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}